{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4d5306-99a4-4305-a356-febdef236643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bcc7f4-d0a6-4a40-83ce-9ad386c803af",
   "metadata": {},
   "source": [
    "In this notebook, we simulate a simple reinforcement learning environment based on a traffic light scenario to demonstrate the Markov property and the basic idea of Q-learning.\n",
    "The environment consists of two possible states — a red light and a green light — and two possible actions for the agent (the driver): stop or go. Each action produces a reward depending on the current state:\n",
    "\n",
    "Going on a red light leads to a strong negative reward (a “penalty”),\n",
    "\n",
    "Stopping at a green light gives a small negative reward (lost time),\n",
    "\n",
    "Going on a green light gives a positive reward.\n",
    "\n",
    "The goal of the agent is to learn, through repeated interaction with the environment, which action maximizes its total long-term reward.\n",
    "The learning process uses the Q-learning algorithm, which updates a table of state–action values (Q-values) according to the rewards received and the expected future rewards.\n",
    "\n",
    "Over time, the agent learns an optimal policy:\n",
    "\n",
    "Stop when the light is red,\n",
    "\n",
    "Go when the light is green.\n",
    "\n",
    "This simple setup illustrates the key principle of Markov Decision Processes (MDPs):\n",
    "\n",
    "The next state and reward depend only on the current state and action — not on the past history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "075ff648-8829-4961-8402-459a8574d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {0:{0:10,1:-5},1:{0:-5,1:10}}; \n",
    "alpha = 0.1; \n",
    "gamma = 0.9; \n",
    "eps = 0.2; \n",
    "states = [0,1]; \n",
    "actions = [0,1]; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b0331b-367d-421c-ab96-9dcf16114809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state,action): \n",
    "    r = rewards[state][action]; \n",
    "    return r; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e9f4552-66fb-4462-bdb9-ed0d48c5568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((2,2)); \n",
    "state = np.random.choice(states)\n",
    "for i in range(100000): \n",
    "    if np.random.uniform()<eps: \n",
    "        action = np.random.choice(actions); \n",
    "    else : action = np.argmax(Q[state]); \n",
    "    reward = step(state,action); \n",
    "    next_state = np.random.choice(states); \n",
    "    Q[state][action] = Q[state][action] + alpha * (reward + Q[next_state][action] * gamma-Q[state][action]); \n",
    "    state = next_state;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65614895-aea1-4fb6-8986-41f0f3c74891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34.38190542, 17.45082546],\n",
       "       [19.90838136, 34.78492062]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11018ec1-26d6-4bc4-8c88-c4932ce21e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
